{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "252aa66c",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-07T20:13:31.490565Z",
     "iopub.status.busy": "2022-03-07T20:13:31.489068Z",
     "iopub.status.idle": "2022-03-07T20:13:32.612012Z",
     "shell.execute_reply": "2022-03-07T20:13:32.611090Z",
     "shell.execute_reply.started": "2022-03-07T18:17:57.304777Z"
    },
    "papermill": {
     "duration": 1.155151,
     "end_time": "2022-03-07T20:13:32.612216",
     "exception": false,
     "start_time": "2022-03-07T20:13:31.457065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3f021e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T20:13:32.720018Z",
     "iopub.status.busy": "2022-03-07T20:13:32.678509Z",
     "iopub.status.idle": "2022-03-07T20:13:32.737723Z",
     "shell.execute_reply": "2022-03-07T20:13:32.738174Z",
     "shell.execute_reply.started": "2022-03-07T18:17:58.246974Z"
    },
    "papermill": {
     "duration": 0.0932,
     "end_time": "2022-03-07T20:13:32.738335",
     "exception": false,
     "start_time": "2022-03-07T20:13:32.645135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series \n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5625095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T20:13:32.803073Z",
     "iopub.status.busy": "2022-03-07T20:13:32.802214Z",
     "iopub.status.idle": "2022-03-07T21:27:33.268604Z",
     "shell.execute_reply": "2022-03-07T21:27:33.267692Z"
    },
    "papermill": {
     "duration": 4440.501225,
     "end_time": "2022-03-07T21:27:33.268794",
     "exception": false,
     "start_time": "2022-03-07T20:13:32.767569",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 30.4min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 73.9min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Read train and test\n",
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe31ade8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:27:33.339272Z",
     "iopub.status.busy": "2022-03-07T21:27:33.338074Z",
     "iopub.status.idle": "2022-03-07T21:27:33.355180Z",
     "shell.execute_reply": "2022-03-07T21:27:33.354700Z"
    },
    "papermill": {
     "duration": 0.055204,
     "end_time": "2022-03-07T21:27:33.355311",
     "exception": false,
     "start_time": "2022-03-07T21:27:33.300107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f771d16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:27:33.416701Z",
     "iopub.status.busy": "2022-03-07T21:27:33.415895Z",
     "iopub.status.idle": "2022-03-07T21:27:33.439934Z",
     "shell.execute_reply": "2022-03-07T21:27:33.439404Z"
    },
    "papermill": {
     "duration": 0.054603,
     "end_time": "2022-03-07T21:27:33.440070",
     "exception": false,
     "start_time": "2022-03-07T21:27:33.385467",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n",
    "\n",
    "# train = pd.read_pickle('../input/train-test-pkl/train_before_colNames.pkl')\n",
    "# test = pd.read_pickle('../input/train-test-pkl/test_before_colNames.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a7369ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:27:33.504898Z",
     "iopub.status.busy": "2022-03-07T21:27:33.504345Z",
     "iopub.status.idle": "2022-03-07T21:27:33.509957Z",
     "shell.execute_reply": "2022-03-07T21:27:33.509510Z"
    },
    "papermill": {
     "duration": 0.040515,
     "end_time": "2022-03-07T21:27:33.510075",
     "exception": false,
     "start_time": "2022-03-07T21:27:33.469560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79547ed3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:27:33.595319Z",
     "iopub.status.busy": "2022-03-07T21:27:33.594567Z",
     "iopub.status.idle": "2022-03-07T21:27:35.493320Z",
     "shell.execute_reply": "2022-03-07T21:27:35.492851Z"
    },
    "papermill": {
     "duration": 1.939379,
     "end_time": "2022-03-07T21:27:35.493445",
     "exception": false,
     "start_time": "2022-03-07T21:27:33.554066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63ba6d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:27:35.570996Z",
     "iopub.status.busy": "2022-03-07T21:27:35.561899Z",
     "iopub.status.idle": "2022-03-07T21:27:35.719294Z",
     "shell.execute_reply": "2022-03-07T21:27:35.718818Z"
    },
    "papermill": {
     "duration": 0.195189,
     "end_time": "2022-03-07T21:27:35.719427",
     "exception": false,
     "start_time": "2022-03-07T21:27:35.524238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "#\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c480a28e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:27:35.790923Z",
     "iopub.status.busy": "2022-03-07T21:27:35.790346Z",
     "iopub.status.idle": "2022-03-07T21:27:43.355005Z",
     "shell.execute_reply": "2022-03-07T21:27:43.354441Z"
    },
    "papermill": {
     "duration": 7.604019,
     "end_time": "2022-03-07T21:27:43.355160",
     "exception": false,
     "start_time": "2022-03-07T21:27:35.751141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] \n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf21213d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:27:43.524993Z",
     "iopub.status.busy": "2022-03-07T21:27:43.523939Z",
     "iopub.status.idle": "2022-03-07T21:27:43.534382Z",
     "shell.execute_reply": "2022-03-07T21:27:43.534986Z"
    },
    "papermill": {
     "duration": 0.146473,
     "end_time": "2022-03-07T21:27:43.535212",
     "exception": false,
     "start_time": "2022-03-07T21:27:43.388739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del mat1,mat2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b2c0546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:27:43.615270Z",
     "iopub.status.busy": "2022-03-07T21:27:43.614622Z",
     "iopub.status.idle": "2022-03-07T21:44:16.287371Z",
     "shell.execute_reply": "2022-03-07T21:44:16.288539Z"
    },
    "papermill": {
     "duration": 992.720343,
     "end_time": "2022-03-07T21:44:16.288779",
     "exception": false,
     "start_time": "2022-03-07T21:27:43.568436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 1\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428698\ttraining's RMSPE: 0.198264\tvalid_1's rmse: 0.00043934\tvalid_1's RMSPE: 0.203917\n",
      "[500]\ttraining's rmse: 0.000406922\ttraining's RMSPE: 0.188193\tvalid_1's rmse: 0.000425343\tvalid_1's RMSPE: 0.19742\n",
      "[750]\ttraining's rmse: 0.000392958\ttraining's RMSPE: 0.181735\tvalid_1's rmse: 0.000416835\tvalid_1's RMSPE: 0.193471\n",
      "[1000]\ttraining's rmse: 0.000383231\ttraining's RMSPE: 0.177236\tvalid_1's rmse: 0.000412414\tvalid_1's RMSPE: 0.191419\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000383231\ttraining's RMSPE: 0.177236\tvalid_1's rmse: 0.000412414\tvalid_1's RMSPE: 0.191419\n",
      "Training fold 2\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000428653\ttraining's RMSPE: 0.19859\tvalid_1's rmse: 0.000440463\tvalid_1's RMSPE: 0.203012\n",
      "[500]\ttraining's rmse: 0.000406379\ttraining's RMSPE: 0.188271\tvalid_1's rmse: 0.000425018\tvalid_1's RMSPE: 0.195894\n",
      "[750]\ttraining's rmse: 0.000392668\ttraining's RMSPE: 0.181918\tvalid_1's rmse: 0.000417108\tvalid_1's RMSPE: 0.192248\n",
      "[1000]\ttraining's rmse: 0.000382831\ttraining's RMSPE: 0.177361\tvalid_1's rmse: 0.000412924\tvalid_1's RMSPE: 0.19032\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000382831\ttraining's RMSPE: 0.177361\tvalid_1's rmse: 0.000412924\tvalid_1's RMSPE: 0.19032\n",
      "Training fold 3\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.00042857\ttraining's RMSPE: 0.198198\tvalid_1's rmse: 0.000465999\tvalid_1's RMSPE: 0.21632\n",
      "[500]\ttraining's rmse: 0.000406477\ttraining's RMSPE: 0.187981\tvalid_1's rmse: 0.000453135\tvalid_1's RMSPE: 0.210349\n",
      "[750]\ttraining's rmse: 0.000392795\ttraining's RMSPE: 0.181653\tvalid_1's rmse: 0.000445963\tvalid_1's RMSPE: 0.207019\n",
      "[1000]\ttraining's rmse: 0.000383092\ttraining's RMSPE: 0.177166\tvalid_1's rmse: 0.000441162\tvalid_1's RMSPE: 0.204791\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000383092\ttraining's RMSPE: 0.177166\tvalid_1's rmse: 0.000441162\tvalid_1's RMSPE: 0.204791\n",
      "Training fold 4\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.00042854\ttraining's RMSPE: 0.198538\tvalid_1's rmse: 0.0004457\tvalid_1's RMSPE: 0.205425\n",
      "[500]\ttraining's rmse: 0.000406238\ttraining's RMSPE: 0.188205\tvalid_1's rmse: 0.00043002\tvalid_1's RMSPE: 0.198198\n",
      "[750]\ttraining's rmse: 0.000392756\ttraining's RMSPE: 0.18196\tvalid_1's rmse: 0.000422586\tvalid_1's RMSPE: 0.194771\n",
      "[1000]\ttraining's rmse: 0.000382971\ttraining's RMSPE: 0.177426\tvalid_1's rmse: 0.000418005\tvalid_1's RMSPE: 0.19266\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000382971\ttraining's RMSPE: 0.177426\tvalid_1's rmse: 0.000418005\tvalid_1's RMSPE: 0.19266\n",
      "Training fold 5\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[250]\ttraining's rmse: 0.000429751\ttraining's RMSPE: 0.198779\tvalid_1's rmse: 0.000442733\tvalid_1's RMSPE: 0.205379\n",
      "[500]\ttraining's rmse: 0.000407766\ttraining's RMSPE: 0.188609\tvalid_1's rmse: 0.000428896\tvalid_1's RMSPE: 0.19896\n",
      "[750]\ttraining's rmse: 0.000394267\ttraining's RMSPE: 0.182366\tvalid_1's rmse: 0.000421757\tvalid_1's RMSPE: 0.195648\n",
      "[1000]\ttraining's rmse: 0.000383985\ttraining's RMSPE: 0.17761\tvalid_1's rmse: 0.000417711\tvalid_1's RMSPE: 0.193771\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttraining's rmse: 0.000383985\ttraining's RMSPE: 0.17761\tvalid_1's rmse: 0.000417711\tvalid_1's RMSPE: 0.193771\n",
      "Our out of folds RMSPE is 0.19466237751890056\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "seed0=2021\n",
    "params0 = {\n",
    "    'objective': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': -1,\n",
    "    'max_bin':100,\n",
    "    'min_data_in_leaf':500,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.72,\n",
    "    'subsample_freq': 4,\n",
    "    'feature_fraction': 0.5,\n",
    "    'lambda_l1': 0.5,\n",
    "    'lambda_l2': 1.0,\n",
    "    'categorical_column':[0],\n",
    "    'seed':seed0,\n",
    "    'feature_fraction_seed': seed0,\n",
    "    'bagging_seed': seed0,\n",
    "    'drop_seed': seed0,\n",
    "    'data_random_seed': seed0,\n",
    "    'n_jobs':-1,\n",
    "    'verbose': -1}\n",
    "seed1=42\n",
    "params1 = {\n",
    "        'learning_rate': 0.1,        \n",
    "        'lambda_l1': 2,\n",
    "        'lambda_l2': 7,\n",
    "        'num_leaves': 800,\n",
    "        'min_sum_hessian_in_leaf': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'feature_fraction_bynode': 0.8,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 42,\n",
    "        'min_data_in_leaf': 700,\n",
    "        'max_depth': 4,\n",
    "        'categorical_column':[0],\n",
    "        'seed': seed1,\n",
    "        'feature_fraction_seed': seed1,\n",
    "        'bagging_seed': seed1,\n",
    "        'drop_seed': seed1,\n",
    "        'data_random_seed': seed1,\n",
    "        'objective': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'n_jobs':-1,\n",
    "    }\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "def train_and_evaluate_lgb(train, test, params):\n",
    "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
    "    y = train['target']\n",
    "    oof_predictions = np.zeros(train.shape[0])\n",
    "    # Create test array to store predictions\n",
    "    test_predictions = np.zeros(test.shape[0])\n",
    "    # Create a KFold object\n",
    "    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n",
    "    # Iterate through each fold\n",
    "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
    "        print(f'Training fold {fold + 1}')\n",
    "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
    "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
    "        # Root mean squared percentage error weights\n",
    "        train_weights = 1 / np.square(y_train)\n",
    "        val_weights = 1 / np.square(y_val)\n",
    "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
    "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
    "        model = lgb.train(params = params,\n",
    "                          num_boost_round=1000,\n",
    "                          train_set = train_dataset, \n",
    "                          valid_sets = [train_dataset, val_dataset], \n",
    "                          verbose_eval = 250,\n",
    "                          early_stopping_rounds=50,\n",
    "                          feval = feval_rmspe)\n",
    "        # Add predictions to the out of folds array\n",
    "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
    "        # Predict the test set\n",
    "        test_predictions += model.predict(test[features]) / 5\n",
    "    rmspe_score = rmspe(y, oof_predictions)\n",
    "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
    "    lgb.plot_importance(model,max_num_features=20)\n",
    "    # Return test predictions\n",
    "    return test_predictions\n",
    "# Traing and evaluate\n",
    "predictions_lgb= train_and_evaluate_lgb(train, test,params0) \n",
    "test['target'] = predictions_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b149577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:44:16.465515Z",
     "iopub.status.busy": "2022-03-07T21:44:16.464228Z",
     "iopub.status.idle": "2022-03-07T21:44:20.907107Z",
     "shell.execute_reply": "2022-03-07T21:44:20.905756Z"
    },
    "papermill": {
     "duration": 4.536035,
     "end_time": "2022-03-07T21:44:20.907290",
     "exception": false,
     "start_time": "2022-03-07T21:44:16.371255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42) \n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def root_mean_squared_per_error(y_true, y_pred):\n",
    "         return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))\n",
    "    \n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=20, verbose=0,\n",
    "    mode='min',restore_best_weights=True)\n",
    "\n",
    "plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=7, verbose=0,\n",
    "    mode='min')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb54897d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:44:21.074731Z",
     "iopub.status.busy": "2022-03-07T21:44:21.073727Z",
     "iopub.status.idle": "2022-03-07T21:44:29.552636Z",
     "shell.execute_reply": "2022-03-07T21:44:29.552152Z"
    },
    "papermill": {
     "duration": 8.572025,
     "end_time": "2022-03-07T21:44:29.552801",
     "exception": false,
     "start_time": "2022-03-07T21:44:20.980776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kfold based on the knn++ algorithm\n",
    "out_train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "#out_train[out_train.isna().any(axis=1)]\n",
    "out_train = out_train.fillna(out_train.mean())\n",
    "out_train.head()\n",
    "\n",
    "# data separation based on knn ++\n",
    "nfolds = 5 # number of folds\n",
    "index = []\n",
    "totDist = []\n",
    "values = []\n",
    "# generates a matriz with the values of \n",
    "mat = out_train.values\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "mat = scaler.fit_transform(mat)\n",
    "\n",
    "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
    "\n",
    "# adds index in the last column\n",
    "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
    "\n",
    "\n",
    "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
    "\n",
    "lineNumber = np.sort(lineNumber)[::-1]\n",
    "\n",
    "for n in range(nfolds):\n",
    "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
    "\n",
    "# saves index\n",
    "for n in range(nfolds):   \n",
    "    values.append([lineNumber[n]])  \n",
    "    \n",
    "s=[]\n",
    "for n in range(nfolds):\n",
    "    s.append(mat[lineNumber[n],:])\n",
    "    \n",
    "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
    "\n",
    "for n in range(nind-1):    \n",
    "\n",
    "    luck = np.random.uniform(0,1,nfolds)\n",
    "    \n",
    "    for cycle in range(nfolds):\n",
    "         # saves the values of index           \n",
    "\n",
    "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
    "\n",
    "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
    "        totDist[cycle] += sumDist        \n",
    "                \n",
    "        # probabilities\n",
    "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
    "        j = 0\n",
    "        kn = 0\n",
    "        for val in f:\n",
    "            j += val        \n",
    "            if (j > luck[cycle]): # the column was selected\n",
    "                break\n",
    "            kn +=1\n",
    "        lineNumber[cycle] = kn\n",
    "        \n",
    "        # delete line of the value added    \n",
    "        for n_iter in range(nfolds):\n",
    "            \n",
    "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
    "            j= 0\n",
    "        \n",
    "        s[cycle] = mat[lineNumber[cycle],:]\n",
    "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
    "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
    "\n",
    "\n",
    "for n_mod in range(nfolds):\n",
    "    values[n_mod] = out_train.index[values[n_mod]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93ae5df6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:44:29.645700Z",
     "iopub.status.busy": "2022-03-07T21:44:29.644123Z",
     "iopub.status.idle": "2022-03-07T21:44:54.002131Z",
     "shell.execute_reply": "2022-03-07T21:44:54.003238Z"
    },
    "papermill": {
     "duration": 24.408489,
     "end_time": "2022-03-07T21:44:54.003451",
     "exception": false,
     "start_time": "2022-03-07T21:44:29.594962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#colNames.remove('row_id')\n",
    "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "qt_train = []\n",
    "train_nn=train[colNames].copy()\n",
    "test_nn=test[colNames].copy()\n",
    "for col in colNames:\n",
    "    #print(col)\n",
    "    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n",
    "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
    "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
    "    qt_train.append(qt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcf83bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:44:54.150994Z",
     "iopub.status.busy": "2022-03-07T21:44:54.149754Z",
     "iopub.status.idle": "2022-03-07T21:44:54.157730Z",
     "shell.execute_reply": "2022-03-07T21:44:54.157278Z"
    },
    "papermill": {
     "duration": 0.085255,
     "end_time": "2022-03-07T21:44:54.157860",
     "exception": false,
     "start_time": "2022-03-07T21:44:54.072605",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
    "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67640dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:44:54.251798Z",
     "iopub.status.busy": "2022-03-07T21:44:54.250688Z",
     "iopub.status.idle": "2022-03-07T21:44:55.757337Z",
     "shell.execute_reply": "2022-03-07T21:44:55.757831Z"
    },
    "papermill": {
     "duration": 1.558914,
     "end_time": "2022-03-07T21:44:55.758150",
     "exception": false,
     "start_time": "2022-03-07T21:44:54.199236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "# making agg features\n",
    "from sklearn.cluster import KMeans\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()\n",
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99c295b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:44:55.853006Z",
     "iopub.status.busy": "2022-03-07T21:44:55.852311Z",
     "iopub.status.idle": "2022-03-07T21:44:55.856267Z",
     "shell.execute_reply": "2022-03-07T21:44:55.855480Z"
    },
    "papermill": {
     "duration": 0.053048,
     "end_time": "2022-03-07T21:44:55.856395",
     "exception": false,
     "start_time": "2022-03-07T21:44:55.803347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66eb10e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:44:55.950228Z",
     "iopub.status.busy": "2022-03-07T21:44:55.948540Z",
     "iopub.status.idle": "2022-03-07T21:44:56.104472Z",
     "shell.execute_reply": "2022-03-07T21:44:56.104981Z"
    },
    "papermill": {
     "duration": 0.205617,
     "end_time": "2022-03-07T21:44:56.105147",
     "exception": false,
     "start_time": "2022-03-07T21:44:55.899530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9090b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:44:56.201273Z",
     "iopub.status.busy": "2022-03-07T21:44:56.200464Z",
     "iopub.status.idle": "2022-03-07T21:45:00.999525Z",
     "shell.execute_reply": "2022-03-07T21:45:01.000164Z"
    },
    "papermill": {
     "duration": 4.850123,
     "end_time": "2022-03-07T21:45:01.000321",
     "exception": false,
     "start_time": "2022-03-07T21:44:56.150198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
    "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
    "del mat1,mat2\n",
    "del train,test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ff2d58d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:45:01.096319Z",
     "iopub.status.busy": "2022-03-07T21:45:01.095512Z",
     "iopub.status.idle": "2022-03-07T21:45:01.109011Z",
     "shell.execute_reply": "2022-03-07T21:45:01.108553Z"
    },
    "papermill": {
     "duration": 0.065076,
     "end_time": "2022-03-07T21:45:01.109134",
     "exception": false,
     "start_time": "2022-03-07T21:45:01.044058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.backend import sigmoid\n",
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers import Activation\n",
    "get_custom_objects().update({'swish': Activation(swish)})\n",
    "\n",
    "hidden_units = (128,64,32)\n",
    "stock_embedding_size = 24\n",
    "\n",
    "cat_data = train_nn['stock_id']\n",
    "\n",
    "def base_model():\n",
    "\n",
    "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
    "    num_input = keras.Input(shape=(244,), name='num_data')\n",
    "\n",
    "\n",
    "    #embedding, flatenning and concatenating\n",
    "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
    "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
    "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
    "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
    "    \n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "\n",
    "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "    \n",
    "    model = keras.Model(\n",
    "    inputs = [stock_id_input, num_input],\n",
    "    outputs = out,\n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "461c9de1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:45:01.201345Z",
     "iopub.status.busy": "2022-03-07T21:45:01.200545Z",
     "iopub.status.idle": "2022-03-07T21:45:01.203508Z",
     "shell.execute_reply": "2022-03-07T21:45:01.203088Z"
    },
    "papermill": {
     "duration": 0.050564,
     "end_time": "2022-03-07T21:45:01.203614",
     "exception": false,
     "start_time": "2022-03-07T21:45:01.153050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65484626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:45:01.311782Z",
     "iopub.status.busy": "2022-03-07T21:45:01.310101Z",
     "iopub.status.idle": "2022-03-07T21:51:04.217317Z",
     "shell.execute_reply": "2022-03-07T21:51:04.216049Z"
    },
    "papermill": {
     "duration": 362.969532,
     "end_time": "2022-03-07T21:51:04.217458",
     "exception": false,
     "start_time": "2022-03-07T21:45:01.247926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 3s 9ms/step - loss: 23.6024 - val_loss: 0.9874\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.3786 - val_loss: 0.5736\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6862 - val_loss: 0.4073\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6613 - val_loss: 0.7404\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6043 - val_loss: 0.5505\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5945 - val_loss: 0.7796\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7331 - val_loss: 0.4999\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6130 - val_loss: 0.2622\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3380 - val_loss: 0.3244\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2871 - val_loss: 0.2192\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2699 - val_loss: 0.2980\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2710 - val_loss: 0.2429\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2818 - val_loss: 0.2260\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2816 - val_loss: 0.2313\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7499 - val_loss: 0.8723\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 1.3718 - val_loss: 0.2304\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2631 - val_loss: 0.2204\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2242 - val_loss: 0.2181\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2236 - val_loss: 0.2185\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2199 - val_loss: 0.2227\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2198 - val_loss: 0.2190\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2224 - val_loss: 0.2219\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2188 - val_loss: 0.2168\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2168 - val_loss: 0.2185\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2201 - val_loss: 0.2186\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2183 - val_loss: 0.2159\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2159 - val_loss: 0.2172\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2181 - val_loss: 0.2144\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2159 - val_loss: 0.2224\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2159 - val_loss: 0.2139\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2168 - val_loss: 0.2161\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2145 - val_loss: 0.2226\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2177 - val_loss: 0.2215\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2156 - val_loss: 0.2428\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2274 - val_loss: 0.2123\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2158 - val_loss: 0.2126\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2143 - val_loss: 0.2119\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2144 - val_loss: 0.2124\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2182 - val_loss: 0.2111\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2138 - val_loss: 0.2117\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2133 - val_loss: 0.2116\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2124 - val_loss: 0.2158\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2138 - val_loss: 0.2122\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2136 - val_loss: 0.2239\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2120 - val_loss: 0.2129\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2173 - val_loss: 0.2143\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2093 - val_loss: 0.2094\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2066 - val_loss: 0.2106\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2097\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.2113\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2083 - val_loss: 0.2098\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2075 - val_loss: 0.2097\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2075 - val_loss: 0.2102\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2074 - val_loss: 0.2102\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2063 - val_loss: 0.2091\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.2094\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2093\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 0.2094\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2093\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2060 - val_loss: 0.2095\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2095\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2056 - val_loss: 0.2090\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2056 - val_loss: 0.2090\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2090\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2056 - val_loss: 0.2091\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2090\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2091\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2091\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2047 - val_loss: 0.2092\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2090\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2090\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2063 - val_loss: 0.2090\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2046 - val_loss: 0.2092\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2048 - val_loss: 0.2091\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.2091\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2053 - val_loss: 0.2090\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2090\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2090\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 0.2090\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2049 - val_loss: 0.2090\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2056 - val_loss: 0.2090\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2047 - val_loss: 0.2090\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2091\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2090\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2090\n",
      "Epoch 86/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2051 - val_loss: 0.2090\n",
      "Epoch 87/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2090\n",
      "Epoch 88/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2052 - val_loss: 0.2090\n",
      "Epoch 89/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2090\n",
      "Epoch 90/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2051 - val_loss: 0.2090\n",
      "Fold 1 NN: 0.20896\n",
      "CV 2/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 10ms/step - loss: 25.6709 - val_loss: 4.0092\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.8196 - val_loss: 0.7324\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7451 - val_loss: 0.3465\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4306 - val_loss: 1.0548\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.9746 - val_loss: 0.8476\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6949 - val_loss: 0.6918\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.7395 - val_loss: 0.4516\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6575 - val_loss: 0.3682\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3858 - val_loss: 0.3033\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2988 - val_loss: 0.2351\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2910 - val_loss: 0.2357\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3082 - val_loss: 0.3338\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3072 - val_loss: 0.3206\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4936 - val_loss: 1.9015\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 3.1265 - val_loss: 0.4275\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3538 - val_loss: 0.2319\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3053 - val_loss: 0.2488\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2294 - val_loss: 0.2740\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2493 - val_loss: 0.2335\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2465 - val_loss: 0.2212\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2325 - val_loss: 0.2194\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2384 - val_loss: 0.2389\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2486 - val_loss: 0.2306\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2512 - val_loss: 0.3293\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2539 - val_loss: 0.2237\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2559 - val_loss: 0.2557\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2502 - val_loss: 0.2707\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2605 - val_loss: 0.2224\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2090 - val_loss: 0.2145\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.2174\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2072 - val_loss: 0.2152\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2070 - val_loss: 0.2144\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2077 - val_loss: 0.2139\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2150\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2131\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2140\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2079 - val_loss: 0.2145\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2067 - val_loss: 0.2131\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2128\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2172\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2228\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2079 - val_loss: 0.2130\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2224\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2085 - val_loss: 0.2153\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2066 - val_loss: 0.2261\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2116 - val_loss: 0.2213\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2027 - val_loss: 0.2132\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.2133\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2123\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2011 - val_loss: 0.2133\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2015 - val_loss: 0.2124\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2115\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2130\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2021 - val_loss: 0.2127\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2135\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2138\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2017 - val_loss: 0.2121\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2012 - val_loss: 0.2147\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2010 - val_loss: 0.2134\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1999 - val_loss: 0.2124\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1995 - val_loss: 0.2128\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1990 - val_loss: 0.2117\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1998 - val_loss: 0.2134\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1992 - val_loss: 0.2129\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1997 - val_loss: 0.2123\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2000 - val_loss: 0.2123\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1991 - val_loss: 0.2121\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1994 - val_loss: 0.2123\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1993 - val_loss: 0.2122\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.1988 - val_loss: 0.2117\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.1988 - val_loss: 0.2122\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.1987 - val_loss: 0.2121\n",
      "Fold 2 NN: 0.21148\n",
      "CV 3/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 6ms/step - loss: 25.1842 - val_loss: 0.8314\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5850 - val_loss: 0.3331\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3649 - val_loss: 0.2814\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3630 - val_loss: 0.3489\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3393 - val_loss: 0.3538\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.9793 - val_loss: 0.5271\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5301 - val_loss: 0.4281\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4545 - val_loss: 0.3997\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.4429 - val_loss: 0.4313\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4045 - val_loss: 0.3220\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2306 - val_loss: 0.2179\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2142 - val_loss: 0.2144\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2136 - val_loss: 0.2170\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2123 - val_loss: 0.2144\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2130 - val_loss: 0.2148\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2119 - val_loss: 0.2183\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2124 - val_loss: 0.2144\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2113 - val_loss: 0.2145\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2130 - val_loss: 0.2202\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2097 - val_loss: 0.2119\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2083 - val_loss: 0.2124\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2082 - val_loss: 0.2118\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2084 - val_loss: 0.2126\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2080 - val_loss: 0.2121\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2082 - val_loss: 0.2119\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2080 - val_loss: 0.2121\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2079 - val_loss: 0.2121\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2073 - val_loss: 0.2121\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2072 - val_loss: 0.2115\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 8ms/step - loss: 0.2062 - val_loss: 0.2117\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2119\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2117\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2119\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2066 - val_loss: 0.2118\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2069 - val_loss: 0.2115\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2117\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2117\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2056 - val_loss: 0.2114\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2114\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2064 - val_loss: 0.2114\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2118\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2072 - val_loss: 0.2115\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2063 - val_loss: 0.2115\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2113\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2114\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.2114\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2059 - val_loss: 0.2114\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2060 - val_loss: 0.2115\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2056 - val_loss: 0.2114\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 0.2114\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2057 - val_loss: 0.2114\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.2114\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2070 - val_loss: 0.2114\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2114\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2114\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2058 - val_loss: 0.2114\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2114\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2061 - val_loss: 0.2114\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2060 - val_loss: 0.2114\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2114\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2061 - val_loss: 0.2114\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2114\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2063 - val_loss: 0.2114\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2053 - val_loss: 0.2114\n",
      "Fold 3 NN: 0.21134\n",
      "CV 4/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 7ms/step - loss: 20.7116 - val_loss: 0.6025\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.7432 - val_loss: 0.5789\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8206 - val_loss: 0.5810\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6639 - val_loss: 0.7168\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8079 - val_loss: 0.5457\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5962 - val_loss: 1.2272\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.9197 - val_loss: 0.2539\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2885 - val_loss: 0.4825\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5857 - val_loss: 0.4898\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.5823 - val_loss: 0.3468\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2522 - val_loss: 0.2594\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2447 - val_loss: 0.2418\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2593 - val_loss: 0.2540\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 1.3423 - val_loss: 0.2823\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2420 - val_loss: 0.2866\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2326 - val_loss: 0.2351\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2333 - val_loss: 0.2258\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2229 - val_loss: 0.2234\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2264 - val_loss: 0.2357\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2311 - val_loss: 0.2915\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2362 - val_loss: 0.2473\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2390 - val_loss: 0.3225\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2463 - val_loss: 0.2402\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2366 - val_loss: 0.2335\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2316 - val_loss: 0.2244\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2084 - val_loss: 0.2151\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2062 - val_loss: 0.2171\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2062 - val_loss: 0.2171\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2063 - val_loss: 0.2169\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2062 - val_loss: 0.2178\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2068 - val_loss: 0.2150\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2054 - val_loss: 0.2169\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2055 - val_loss: 0.2188\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2044 - val_loss: 0.2151\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2033 - val_loss: 0.2156\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2039 - val_loss: 0.2154\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2038 - val_loss: 0.2150\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2031 - val_loss: 0.2149\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2033 - val_loss: 0.2151\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2039 - val_loss: 0.2150\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2035 - val_loss: 0.2150\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2033 - val_loss: 0.2160\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2035 - val_loss: 0.2149\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.2166\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2030 - val_loss: 0.2143\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2037 - val_loss: 0.2163\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2165\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2164\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2030 - val_loss: 0.2151\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2033 - val_loss: 0.2160\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2164\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2029 - val_loss: 0.2157\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2021 - val_loss: 0.2151\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.2151\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2025 - val_loss: 0.2157\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2152\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2148\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.2147\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2013 - val_loss: 0.2153\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.2150\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2150\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2149\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2148\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2013 - val_loss: 0.2150\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2149\n",
      "Fold 4 NN: 0.21426\n",
      "CV 5/5\n",
      "Epoch 1/1000\n",
      "168/168 [==============================] - 2s 9ms/step - loss: 32.7981 - val_loss: 1.6085\n",
      "Epoch 2/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 1.0053 - val_loss: 0.6722\n",
      "Epoch 3/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.6021 - val_loss: 0.3151\n",
      "Epoch 4/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.3334 - val_loss: 0.7522\n",
      "Epoch 5/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.6456 - val_loss: 0.2519\n",
      "Epoch 6/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2845 - val_loss: 0.2386\n",
      "Epoch 7/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2921 - val_loss: 0.3064\n",
      "Epoch 8/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2993 - val_loss: 0.2276\n",
      "Epoch 9/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3121 - val_loss: 0.3146\n",
      "Epoch 10/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.4320 - val_loss: 2.6237\n",
      "Epoch 11/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.8322 - val_loss: 0.2597\n",
      "Epoch 12/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2668 - val_loss: 0.2275\n",
      "Epoch 13/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2492 - val_loss: 0.3495\n",
      "Epoch 14/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2724 - val_loss: 0.2250\n",
      "Epoch 15/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.9440 - val_loss: 0.3273\n",
      "Epoch 16/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2927 - val_loss: 0.2273\n",
      "Epoch 17/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2318 - val_loss: 0.2593\n",
      "Epoch 18/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2395 - val_loss: 0.3234\n",
      "Epoch 19/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2419 - val_loss: 0.6967\n",
      "Epoch 20/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.6006 - val_loss: 0.3211\n",
      "Epoch 21/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.3160 - val_loss: 0.2471\n",
      "Epoch 22/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2143 - val_loss: 0.2157\n",
      "Epoch 23/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2095 - val_loss: 0.2160\n",
      "Epoch 24/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2079 - val_loss: 0.2163\n",
      "Epoch 25/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2098 - val_loss: 0.2160\n",
      "Epoch 26/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2081 - val_loss: 0.2162\n",
      "Epoch 27/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2081 - val_loss: 0.2336\n",
      "Epoch 28/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2093 - val_loss: 0.2174\n",
      "Epoch 29/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2074 - val_loss: 0.2146\n",
      "Epoch 30/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2069 - val_loss: 0.2170\n",
      "Epoch 31/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2063 - val_loss: 0.2205\n",
      "Epoch 32/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2132 - val_loss: 0.2153\n",
      "Epoch 33/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2070 - val_loss: 0.2181\n",
      "Epoch 34/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2067 - val_loss: 0.2220\n",
      "Epoch 35/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2081 - val_loss: 0.2179\n",
      "Epoch 36/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2082 - val_loss: 0.2193\n",
      "Epoch 37/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2044 - val_loss: 0.2140\n",
      "Epoch 38/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2034 - val_loss: 0.2148\n",
      "Epoch 39/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2027 - val_loss: 0.2154\n",
      "Epoch 40/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2138\n",
      "Epoch 41/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2028 - val_loss: 0.2140\n",
      "Epoch 42/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2031 - val_loss: 0.2138\n",
      "Epoch 43/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2029 - val_loss: 0.2145\n",
      "Epoch 44/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2031 - val_loss: 0.2145\n",
      "Epoch 45/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.2137\n",
      "Epoch 46/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2032 - val_loss: 0.2136\n",
      "Epoch 47/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2035 - val_loss: 0.2160\n",
      "Epoch 48/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2042 - val_loss: 0.2229\n",
      "Epoch 49/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2223\n",
      "Epoch 50/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2155\n",
      "Epoch 51/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2035 - val_loss: 0.2141\n",
      "Epoch 52/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2041 - val_loss: 0.2155\n",
      "Epoch 53/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2021 - val_loss: 0.2137\n",
      "Epoch 54/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2014 - val_loss: 0.2136\n",
      "Epoch 55/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2026 - val_loss: 0.2147\n",
      "Epoch 56/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2141\n",
      "Epoch 57/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2013 - val_loss: 0.2139\n",
      "Epoch 58/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2023 - val_loss: 0.2145\n",
      "Epoch 59/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2005 - val_loss: 0.2138\n",
      "Epoch 60/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2137\n",
      "Epoch 61/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2134\n",
      "Epoch 62/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2015 - val_loss: 0.2135\n",
      "Epoch 63/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2135\n",
      "Epoch 64/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2135\n",
      "Epoch 65/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2134\n",
      "Epoch 66/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2140\n",
      "Epoch 67/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2010 - val_loss: 0.2136\n",
      "Epoch 68/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2013 - val_loss: 0.2137\n",
      "Epoch 69/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2013 - val_loss: 0.2137\n",
      "Epoch 70/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2135\n",
      "Epoch 71/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2136\n",
      "Epoch 72/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2006 - val_loss: 0.2135\n",
      "Epoch 73/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2020 - val_loss: 0.2138\n",
      "Epoch 74/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2009 - val_loss: 0.2135\n",
      "Epoch 75/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2012 - val_loss: 0.2137\n",
      "Epoch 76/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2010 - val_loss: 0.2137\n",
      "Epoch 77/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2013 - val_loss: 0.2136\n",
      "Epoch 78/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2005 - val_loss: 0.2136\n",
      "Epoch 79/1000\n",
      "168/168 [==============================] - 1s 7ms/step - loss: 0.2005 - val_loss: 0.2136\n",
      "Epoch 80/1000\n",
      "168/168 [==============================] - 1s 6ms/step - loss: 0.2010 - val_loss: 0.2136\n",
      "Epoch 81/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2004 - val_loss: 0.2136\n",
      "Epoch 82/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2008 - val_loss: 0.2136\n",
      "Epoch 83/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2018 - val_loss: 0.2136\n",
      "Epoch 84/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2011 - val_loss: 0.2136\n",
      "Epoch 85/1000\n",
      "168/168 [==============================] - 1s 5ms/step - loss: 0.2010 - val_loss: 0.2136\n",
      "Fold 5 NN: 0.21336\n"
     ]
    }
   ],
   "source": [
    "target_name='target'\n",
    "scores_folds = {}\n",
    "model_name = 'NN'\n",
    "pred_name = 'pred_{}'.format(model_name)\n",
    "\n",
    "n_folds = 5\n",
    "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\n",
    "scores_folds[model_name] = []\n",
    "counter = 1\n",
    "\n",
    "features_to_consider = list(train_nn)\n",
    "\n",
    "features_to_consider.remove('time_id')\n",
    "features_to_consider.remove('target')\n",
    "try:\n",
    "    features_to_consider.remove('pred_NN')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
    "\n",
    "train_nn[pred_name] = 0\n",
    "test_nn[target_name] = 0\n",
    "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
    "\n",
    "import tensorflow as tf\n",
    "for n_count in range(n_folds):\n",
    "    print('CV {}/{}'.format(counter, n_folds))\n",
    "    \n",
    "    indexes = np.arange(nfolds).astype(int)    \n",
    "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
    "    \n",
    "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
    "    \n",
    "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
    "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
    "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
    "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
    "    \n",
    "    #############################################################################################\n",
    "    # NN\n",
    "    #############################################################################################\n",
    "    \n",
    "    model = base_model()\n",
    "    \n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=0.006),\n",
    "        loss=root_mean_squared_per_error\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        features_to_consider.remove('stock_id')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    num_data = X_train[features_to_consider]\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
    "    num_data = scaler.fit_transform(num_data.values)    \n",
    "    \n",
    "    cat_data = X_train['stock_id']    \n",
    "    target =  y_train\n",
    "    \n",
    "    num_data_test = X_test[features_to_consider]\n",
    "    num_data_test = scaler.transform(num_data_test.values)\n",
    "    cat_data_test = X_test['stock_id']\n",
    "\n",
    "    model.fit([cat_data, num_data], \n",
    "              target,               \n",
    "              batch_size=2048,\n",
    "              epochs=1000,\n",
    "              validation_data=([cat_data_test, num_data_test], y_test),\n",
    "              callbacks=[es, plateau],\n",
    "              validation_batch_size=len(y_test),\n",
    "              shuffle=True,\n",
    "             verbose = 1)\n",
    "\n",
    "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
    "    \n",
    "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
    "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
    "    scores_folds[model_name].append(score)\n",
    "    \n",
    "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
    "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
    "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
    "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
    "       \n",
    "    counter += 1\n",
    "    features_to_consider.append('stock_id')\n",
    "    \n",
    "    tf.keras.utils.plot_model(\n",
    "    model, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0f85f1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:51:07.621198Z",
     "iopub.status.busy": "2022-03-07T21:51:07.619575Z",
     "iopub.status.idle": "2022-03-07T21:51:07.899582Z",
     "shell.execute_reply": "2022-03-07T21:51:07.900691Z"
    },
    "papermill": {
     "duration": 2.10361,
     "end_time": "2022-03-07T21:51:07.900894",
     "exception": false,
     "start_time": "2022-03-07T21:51:05.797284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23772"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del train_nn\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4e2d8",
   "metadata": {
    "papermill": {
     "duration": 1.541364,
     "end_time": "2022-03-07T21:51:11.101931",
     "exception": false,
     "start_time": "2022-03-07T21:51:09.560567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67731471",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:51:14.205106Z",
     "iopub.status.busy": "2022-03-07T21:51:14.204432Z",
     "iopub.status.idle": "2022-03-07T21:51:15.233130Z",
     "shell.execute_reply": "2022-03-07T21:51:15.232306Z"
    },
    "papermill": {
     "duration": 2.590284,
     "end_time": "2022-03-07T21:51:15.233263",
     "exception": false,
     "start_time": "2022-03-07T21:51:12.642979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import random\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "SEED = 42\n",
    "\n",
    "import random\n",
    "\n",
    "def random_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "random_seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# train = pd.read_pickle('../input/train-test-pkl/train_before_colNames.pkl')\n",
    "# test = pd.read_pickle('../input/train-test-pkl/test_before_colNames.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "415f9159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:51:18.387803Z",
     "iopub.status.busy": "2022-03-07T21:51:18.386903Z",
     "iopub.status.idle": "2022-03-07T21:51:18.389594Z",
     "shell.execute_reply": "2022-03-07T21:51:18.389180Z"
    },
    "papermill": {
     "duration": 1.563569,
     "end_time": "2022-03-07T21:51:18.389729",
     "exception": false,
     "start_time": "2022-03-07T21:51:16.826160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "\n",
    "\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from sklearn import preprocessing, model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy.matlib\n",
    "\n",
    "\n",
    "path_submissions = '/'\n",
    "\n",
    "target_name = 'target'\n",
    "scores_folds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cb98f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:51:21.816019Z",
     "iopub.status.busy": "2022-03-07T21:51:21.805658Z",
     "iopub.status.idle": "2022-03-07T21:51:21.831375Z",
     "shell.execute_reply": "2022-03-07T21:51:21.830930Z"
    },
    "papermill": {
     "duration": 1.630882,
     "end_time": "2022-03-07T21:51:21.831500",
     "exception": false,
     "start_time": "2022-03-07T21:51:20.200618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directory\n",
    "data_dir = '../input/optiver-realized-volatility-prediction/'\n",
    "\n",
    "# Function to calculate first WAP\n",
    "def calc_wap1(df):\n",
    "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate second WAP\n",
    "def calc_wap2(df):\n",
    "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap3(df):\n",
    "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
    "    return wap\n",
    "\n",
    "def calc_wap4(df):\n",
    "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
    "    return wap\n",
    "\n",
    "# Function to calculate the log of the return\n",
    "# Remember that logb(x / y) = logb(x) - logb(y)\n",
    "def log_return(series):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "# Calculate the realized volatility\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series**2))\n",
    "\n",
    "# Function to count unique elements of a series\n",
    "def count_unique(series):\n",
    "    return len(np.unique(series))\n",
    "\n",
    "# Function to read our base train and test set\n",
    "def read_train_test():\n",
    "    train = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "    test = pd.read_csv('../input/optiver-realized-volatility-prediction/test.csv')\n",
    "    # Create a key to merge with book and trade data\n",
    "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
    "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
    "    print(f'Our training set has {train.shape[0]} rows')\n",
    "    return train, test\n",
    "\n",
    "# Function to preprocess book data (for each stock id)\n",
    "def book_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    # Calculate Wap\n",
    "    df['wap1'] = calc_wap1(df)\n",
    "    df['wap2'] = calc_wap2(df)\n",
    "    df['wap3'] = calc_wap3(df)\n",
    "    df['wap4'] = calc_wap4(df)\n",
    "    # Calculate log returns\n",
    "    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n",
    "    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n",
    "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
    "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
    "    # Calculate wap balance\n",
    "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
    "    # Calculate spread\n",
    "    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
    "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
    "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
    "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
    "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
    "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
    "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
    "    \n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'wap1': [np.sum, np.std],\n",
    "        'wap2': [np.sum, np.std],\n",
    "        'wap3': [np.sum, np.std],\n",
    "        'wap4': [np.sum, np.std],\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "        'wap_balance': [np.sum, np.max],\n",
    "        'price_spread':[np.sum, np.max],\n",
    "        'price_spread2':[np.sum, np.max],\n",
    "        'bid_spread':[np.sum, np.max],\n",
    "        'ask_spread':[np.sum, np.max],\n",
    "        'total_volume':[np.sum, np.max],\n",
    "        'volume_imbalance':[np.sum, np.max],\n",
    "        \"bid_ask_spread\":[np.sum,  np.max],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return1': [realized_volatility],\n",
    "        'log_return2': [realized_volatility],\n",
    "        'log_return3': [realized_volatility],\n",
    "        'log_return4': [realized_volatility],\n",
    "    }\n",
    "    \n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "\n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    # Create row_id so we can merge\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
    "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to preprocess trade data (for each stock id)\n",
    "def trade_preprocessor(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
    "    df['amount']=df['price']*df['size']\n",
    "    # Dict for aggregations\n",
    "    create_feature_dict = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum, np.max, np.min],\n",
    "        'order_count':[np.sum,np.max],\n",
    "        'amount':[np.sum,np.max,np.min],\n",
    "    }\n",
    "    create_feature_dict_time = {\n",
    "        'log_return':[realized_volatility],\n",
    "        'seconds_in_bucket':[count_unique],\n",
    "        'size':[np.sum],\n",
    "        'order_count':[np.sum],\n",
    "    }\n",
    "    # Function to get group stats for different windows (seconds in bucket)\n",
    "    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n",
    "        # Group by the window\n",
    "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n",
    "        # Rename columns joining suffix\n",
    "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
    "        # Add a suffix to differentiate windows\n",
    "        if add_suffix:\n",
    "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
    "        return df_feature\n",
    "    \n",
    "\n",
    "    # Get the stats for different windows\n",
    "    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n",
    "    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n",
    "    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n",
    "    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n",
    "    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n",
    "    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n",
    "    \n",
    "    def tendency(price, vol):    \n",
    "        df_diff = np.diff(price)\n",
    "        val = (df_diff/price[1:])*100\n",
    "        power = np.sum(val*vol[1:])\n",
    "        return(power)\n",
    "    \n",
    "    lis = []\n",
    "    for n_time_id in df['time_id'].unique():\n",
    "        df_id = df[df['time_id'] == n_time_id]        \n",
    "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
    "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
    "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
    "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
    "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
    "        # new\n",
    "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
    "        energy = np.mean(df_id['price'].values**2)\n",
    "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
    "        \n",
    "        # vol vars\n",
    "        \n",
    "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
    "        energy_v = np.sum(df_id['size'].values**2)\n",
    "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
    "        \n",
    "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
    "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
    "    \n",
    "    df_lr = pd.DataFrame(lis)\n",
    "        \n",
    "   \n",
    "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
    "    \n",
    "    # Merge all\n",
    "    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n",
    "    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n",
    "    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
    "    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n",
    "    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n",
    "    # Drop unnecesary time_ids\n",
    "    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n",
    "    \n",
    "    \n",
    "    df_feature = df_feature.add_prefix('trade_')\n",
    "    stock_id = file_path.split('=')[1]\n",
    "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
    "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
    "    return df_feature\n",
    "\n",
    "# Function to get group stats for the stock_id and time_id\n",
    "def get_time_stock(df):\n",
    "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n",
    "                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n",
    "                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n",
    "\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
    "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
    "\n",
    "    # Group by the stock id\n",
    "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n",
    "    # Rename columns joining suffix\n",
    "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
    "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
    "    \n",
    "    # Merge with original dataframe\n",
    "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
    "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
    "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
    "    return df\n",
    "    \n",
    "# Funtion to make preprocessing function in parallel (for each stock id)\n",
    "def preprocessor(list_stock_ids, is_train = True):\n",
    "    \n",
    "    # Parrallel for loop\n",
    "    def for_joblib(stock_id):\n",
    "        # Train\n",
    "        if is_train:\n",
    "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
    "        # Test\n",
    "        else:\n",
    "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
    "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
    "    \n",
    "        # Preprocess book and trade data and merge them\n",
    "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
    "        \n",
    "        # Return the merge dataframe\n",
    "        return df_tmp\n",
    "    \n",
    "    # Use parallel api to call paralle for loop\n",
    "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
    "    # Concatenate all the dataframes that return from Parallel\n",
    "    df = pd.concat(df, ignore_index = True)\n",
    "    return df\n",
    "\n",
    "# Function to calculate the root mean squared percentage error\n",
    "def rmspe(y_true, y_pred):\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
    "\n",
    "# Function to early stop with root mean squared percentage error\n",
    "def feval_rmspe(y_pred, lgb_train):\n",
    "    y_true = lgb_train.get_label()\n",
    "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def random_seed(SEED):\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "random_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db312461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T21:51:25.005583Z",
     "iopub.status.busy": "2022-03-07T21:51:25.004541Z",
     "iopub.status.idle": "2022-03-07T23:05:30.568695Z",
     "shell.execute_reply": "2022-03-07T23:05:30.568187Z"
    },
    "papermill": {
     "duration": 4447.20173,
     "end_time": "2022-03-07T23:05:30.568844",
     "exception": false,
     "start_time": "2022-03-07T21:51:23.367114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training set has 428932 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed: 30.9min\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed: 74.0min finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "train, test = read_train_test()\n",
    "\n",
    "# Get unique stock ids \n",
    "train_stock_ids = train['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "train_ = preprocessor(train_stock_ids, is_train = True)\n",
    "train = train.merge(train_, on = ['row_id'], how = 'left')\n",
    "\n",
    "# Get unique stock ids \n",
    "test_stock_ids = test['stock_id'].unique()\n",
    "# Preprocess them using Parallel and our single stock id functions\n",
    "test_ = preprocessor(test_stock_ids, is_train = False)\n",
    "test = test.merge(test_, on = ['row_id'], how = 'left') \n",
    "\n",
    "# Get group stats of time_id and stock_id\n",
    "train = get_time_stock(train)\n",
    "test = get_time_stock(test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "226d14c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:05:33.836848Z",
     "iopub.status.busy": "2022-03-07T23:05:33.836003Z",
     "iopub.status.idle": "2022-03-07T23:05:33.858721Z",
     "shell.execute_reply": "2022-03-07T23:05:33.859226Z"
    },
    "papermill": {
     "duration": 1.752106,
     "end_time": "2022-03-07T23:05:33.859371",
     "exception": false,
     "start_time": "2022-03-07T23:05:32.107265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# replace by order sum (tau)\n",
    "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
    "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
    "#train['size_tau_450'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_450'] )\n",
    "#test['size_tau_450'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_450'] )\n",
    "train['size_tau_400'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_400'] )\n",
    "test['size_tau_400'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_400'] )\n",
    "train['size_tau_300'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_300'] )\n",
    "test['size_tau_300'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_300'] )\n",
    "#train['size_tau_150'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_150'] )\n",
    "#test['size_tau_150'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_150'] )\n",
    "train['size_tau_200'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_200'] )\n",
    "test['size_tau_200'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_200'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "080dc3ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:05:37.045194Z",
     "iopub.status.busy": "2022-03-07T23:05:37.044112Z",
     "iopub.status.idle": "2022-03-07T23:05:37.060627Z",
     "shell.execute_reply": "2022-03-07T23:05:37.061137Z"
    },
    "papermill": {
     "duration": 1.547382,
     "end_time": "2022-03-07T23:05:37.061292",
     "exception": false,
     "start_time": "2022-03-07T23:05:35.513910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
    "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_450'] = np.sqrt( 0.25/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_450'] = np.sqrt( 0.25/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_400'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_400'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_300'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_300'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
    "#train['size_tau2_150'] = np.sqrt( 0.75/ train['trade_order_count_sum'] )\n",
    "#test['size_tau2_150'] = np.sqrt( 0.75/ test['trade_order_count_sum'] )\n",
    "train['size_tau2_200'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
    "test['size_tau2_200'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
    "\n",
    "# delta tau\n",
    "train['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n",
    "test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a13f8899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:05:40.163987Z",
     "iopub.status.busy": "2022-03-07T23:05:40.163279Z",
     "iopub.status.idle": "2022-03-07T23:05:40.166927Z",
     "shell.execute_reply": "2022-03-07T23:05:40.166447Z"
    },
    "papermill": {
     "duration": 1.576081,
     "end_time": "2022-03-07T23:05:40.167040",
     "exception": false,
     "start_time": "2022-03-07T23:05:38.590959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colNames = [col for col in list(train.columns)\n",
    "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
    "len(colNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55eaea9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:05:43.243365Z",
     "iopub.status.busy": "2022-03-07T23:05:43.242756Z",
     "iopub.status.idle": "2022-03-07T23:05:44.900708Z",
     "shell.execute_reply": "2022-03-07T23:05:44.901743Z"
    },
    "papermill": {
     "duration": 3.196534,
     "end_time": "2022-03-07T23:05:44.901951",
     "exception": false,
     "start_time": "2022-03-07T23:05:41.705417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 4 2 1 1 2 4 6 2 1 0 4 4 1 1 1 2 4 4 4 0 1 1 3 1 1 4 3 4 3 4 4 1 3 3 4\n",
      " 3 4 1 4 1 4 4 1 0 4 4 1 0 0 3 3 3 2 0 2 4 1 4 4 1 4 1 0 3 3 0 3 0 6 5 3 3\n",
      " 0 1 2 0 3 3 3 4 1 1 0 2 3 3 1 0 1 4 4 4 4 4 1 3 1 0 1 4 1 0 1 4 1 0 4 0 4\n",
      " 0]\n",
      "[1, 11, 22, 50, 55, 56, 62, 73, 76, 78, 84, 87, 96, 101, 112, 116, 122, 124, 126]\n",
      "[0, 4, 5, 10, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 53, 66, 69, 72, 85, 94, 95, 100, 102, 109, 111, 113, 115, 118, 120]\n",
      "[3, 6, 9, 18, 61, 63, 86, 97]\n",
      "[27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 75, 77, 82, 83, 88, 89, 90, 98, 99, 110]\n",
      "[2, 7, 13, 14, 19, 20, 21, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 64, 67, 68, 70, 93, 103, 104, 105, 107, 108, 114, 119, 123, 125]\n",
      "[81]\n",
      "[8, 80]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# making agg features\n",
    "\n",
    "train_p = pd.read_csv('../input/optiver-realized-volatility-prediction/train.csv')\n",
    "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
    "\n",
    "corr = train_p.corr()\n",
    "\n",
    "ids = corr.index\n",
    "\n",
    "kmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n",
    "print(kmeans.labels_)\n",
    "\n",
    "l = []\n",
    "for n in range(7):\n",
    "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
    "    \n",
    "\n",
    "mat = []\n",
    "matTest = []\n",
    "\n",
    "n = 0\n",
    "for ind in l:\n",
    "    print(ind)\n",
    "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    mat.append ( newDf )\n",
    "    \n",
    "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
    "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
    "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
    "    matTest.append ( newDf )\n",
    "    \n",
    "    n+=1\n",
    "    \n",
    "mat1 = pd.concat(mat).reset_index()\n",
    "mat1.drop(columns=['target'],inplace=True)\n",
    "\n",
    "mat2 = pd.concat(matTest).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b8090eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:05:48.068519Z",
     "iopub.status.busy": "2022-03-07T23:05:48.067724Z",
     "iopub.status.idle": "2022-03-07T23:05:48.224409Z",
     "shell.execute_reply": "2022-03-07T23:05:48.223953Z"
    },
    "papermill": {
     "duration": 1.688075,
     "end_time": "2022-03-07T23:05:48.224536",
     "exception": false,
     "start_time": "2022-03-07T23:05:46.536461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
    "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
    "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
    "mat1.reset_index(inplace=True)\n",
    "\n",
    "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
    "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
    "mat2.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50d9cca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:05:51.848391Z",
     "iopub.status.busy": "2022-03-07T23:05:51.847832Z",
     "iopub.status.idle": "2022-03-07T23:05:56.522155Z",
     "shell.execute_reply": "2022-03-07T23:05:56.521488Z"
    },
    "papermill": {
     "duration": 6.26602,
     "end_time": "2022-03-07T23:05:56.522296",
     "exception": false,
     "start_time": "2022-03-07T23:05:50.256276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nnn = ['time_id',\n",
    "     'log_return1_realized_volatility_0c1',\n",
    "     'log_return1_realized_volatility_1c1',     \n",
    "     'log_return1_realized_volatility_3c1',\n",
    "     'log_return1_realized_volatility_4c1',     \n",
    "     'log_return1_realized_volatility_6c1',\n",
    "     'total_volume_sum_0c1',\n",
    "     'total_volume_sum_1c1', \n",
    "     'total_volume_sum_3c1',\n",
    "     'total_volume_sum_4c1', \n",
    "     'total_volume_sum_6c1',\n",
    "     'trade_size_sum_0c1',\n",
    "     'trade_size_sum_1c1', \n",
    "     'trade_size_sum_3c1',\n",
    "     'trade_size_sum_4c1', \n",
    "     'trade_size_sum_6c1',\n",
    "     'trade_order_count_sum_0c1',\n",
    "     'trade_order_count_sum_1c1',\n",
    "     'trade_order_count_sum_3c1',\n",
    "     'trade_order_count_sum_4c1',\n",
    "     'trade_order_count_sum_6c1',      \n",
    "     'price_spread_sum_0c1',\n",
    "     'price_spread_sum_1c1',\n",
    "     'price_spread_sum_3c1',\n",
    "     'price_spread_sum_4c1',\n",
    "     'price_spread_sum_6c1',   \n",
    "     'bid_spread_sum_0c1',\n",
    "     'bid_spread_sum_1c1',\n",
    "     'bid_spread_sum_3c1',\n",
    "     'bid_spread_sum_4c1',\n",
    "     'bid_spread_sum_6c1',       \n",
    "     'ask_spread_sum_0c1',\n",
    "     'ask_spread_sum_1c1',\n",
    "     'ask_spread_sum_3c1',\n",
    "     'ask_spread_sum_4c1',\n",
    "     'ask_spread_sum_6c1',   \n",
    "     'volume_imbalance_sum_0c1',\n",
    "     'volume_imbalance_sum_1c1',\n",
    "     'volume_imbalance_sum_3c1',\n",
    "     'volume_imbalance_sum_4c1',\n",
    "     'volume_imbalance_sum_6c1',       \n",
    "     'bid_ask_spread_sum_0c1',\n",
    "     'bid_ask_spread_sum_1c1',\n",
    "     'bid_ask_spread_sum_3c1',\n",
    "     'bid_ask_spread_sum_4c1',\n",
    "     'bid_ask_spread_sum_6c1',\n",
    "     'size_tau2_0c1',\n",
    "     'size_tau2_1c1',\n",
    "     'size_tau2_3c1',\n",
    "     'size_tau2_4c1',\n",
    "     'size_tau2_6c1'] \n",
    "train = pd.merge(train,mat1[nnn],how='left',on='time_id')\n",
    "test = pd.merge(test,mat2[nnn],how='left',on='time_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2aca121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:05:59.643470Z",
     "iopub.status.busy": "2022-03-07T23:05:59.642593Z",
     "iopub.status.idle": "2022-03-07T23:06:04.003933Z",
     "shell.execute_reply": "2022-03-07T23:06:04.002789Z"
    },
    "papermill": {
     "duration": 5.913548,
     "end_time": "2022-03-07T23:06:04.004080",
     "exception": false,
     "start_time": "2022-03-07T23:05:58.090532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in train.columns.to_list()[4:]:\n",
    "    train[col] = train[col].fillna(train[col].mean())\n",
    "    \n",
    "scales = train.drop(['row_id','target','time_id','stock_id'],axis=1).columns.to_list()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[scales])\n",
    "scaler_name = 'scaler'\n",
    "pickle.dump(scaler,open(scaler_name,'wb'))\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le=LabelEncoder()\n",
    "le.fit(train['stock_id'])\n",
    "train['stock_id'] = le.transform(train['stock_id'])\n",
    "with open ('stock_id_encoder.txt','wb') as f:\n",
    "    pickle.dump(le,f)\n",
    "\n",
    "def create_folds(data, num_splits, target):\n",
    "    data['kfold'] = -1\n",
    "    data = data.sample(frac = 1).reset_index(drop=True)\n",
    "    num_bins=int(np.floor(1 + np.log2(len(data))))\n",
    "    data.loc[:,\"bins\"] = pd.cut(data[target], bins=num_bins,labels=False )\n",
    "    kf=model_selection.StratifiedKFold(n_splits=num_splits)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
    "        data.loc[v_, 'kfold'] = f\n",
    "\n",
    "        # drop the bins column\n",
    "    data = data.drop(\"bins\", axis=1)\n",
    "\n",
    "    # return dataframe with folds\n",
    "    return data\n",
    "\n",
    "\n",
    "train = create_folds(train, 5, \"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "075e1aba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:06:07.358898Z",
     "iopub.status.busy": "2022-03-07T23:06:07.358248Z",
     "iopub.status.idle": "2022-03-07T23:06:14.700901Z",
     "shell.execute_reply": "2022-03-07T23:06:14.700339Z"
    },
    "papermill": {
     "duration": 9.162822,
     "end_time": "2022-03-07T23:06:14.701050",
     "exception": false,
     "start_time": "2022-03-07T23:06:05.538228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (0.23.2)\r\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (1.6.3)\r\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (1.19.5)\r\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (4.61.1)\r\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch-tabnet==3.1.1) (1.7.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==3.1.1) (1.0.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch-tabnet==3.1.1) (2.1.0)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==3.1.1) (0.18.2)\r\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==3.1.1) (3.7.4.3)\r\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch-tabnet==3.1.1) (0.6)\r\n",
      "Installing collected packages: pytorch-tabnet\r\n",
      "Successfully installed pytorch-tabnet-3.1.1\r\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/pytorchtabnet/pytorch_tabnet-3.1.1-py3-none-any.whl\n",
    "# TabNet\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from pytorch_tabnet.metrics import Metric\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def rmspe(y_true, y_pred):\n",
    "\n",
    "    if (y_true == 0).any():\n",
    "        raise ValueError(\"Root Mean Square Percentage Error cannot be used when \"\n",
    "                         \"targets contain zero values.\")\n",
    "\n",
    "    loss = np.sqrt(np.mean(np.square(((y_true - y_pred) / y_true)), axis=0)).item()\n",
    "\n",
    "    return loss\n",
    "\n",
    "class RMSPE(Metric):\n",
    "    def __init__(self):\n",
    "        self._name = \"rmspe\"\n",
    "        self._maximize=False\n",
    "\n",
    "    def __call__(self, y_true,y_score):\n",
    "        return rmspe(y_true,y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "343e8ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-07T23:06:18.034329Z",
     "iopub.status.busy": "2022-03-07T23:06:18.033340Z",
     "iopub.status.idle": "2022-03-08T00:28:48.615142Z",
     "shell.execute_reply": "2022-03-08T00:28:48.614679Z"
    },
    "papermill": {
     "duration": 4952.365414,
     "end_time": "2022-03-08T00:28:48.615273",
     "exception": false,
     "start_time": "2022-03-07T23:06:16.249859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Fold:0--------start----\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.09226 | train_rmspe: 0.69746 | valid_rmspe: 0.64583 |  0:00:25s\n",
      "epoch 1  | loss: 0.00112 | train_rmspe: 0.62591 | valid_rmspe: 0.60417 |  0:00:52s\n",
      "epoch 2  | loss: 0.00061 | train_rmspe: 0.50867 | valid_rmspe: 0.51451 |  0:01:18s\n",
      "epoch 3  | loss: 0.00055 | train_rmspe: 0.37174 | valid_rmspe: 0.36086 |  0:01:45s\n",
      "epoch 4  | loss: 0.00046 | train_rmspe: 0.30357 | valid_rmspe: 0.29746 |  0:02:11s\n",
      "epoch 5  | loss: 0.00042 | train_rmspe: 0.29821 | valid_rmspe: 0.29543 |  0:02:38s\n",
      "epoch 6  | loss: 0.00041 | train_rmspe: 0.27786 | valid_rmspe: 0.27776 |  0:03:04s\n",
      "epoch 7  | loss: 0.00038 | train_rmspe: 0.28098 | valid_rmspe: 0.28544 |  0:03:31s\n",
      "epoch 8  | loss: 0.00037 | train_rmspe: 0.28251 | valid_rmspe: 0.29597 |  0:03:57s\n",
      "epoch 9  | loss: 0.00036 | train_rmspe: 0.27257 | valid_rmspe: 0.27606 |  0:04:23s\n",
      "epoch 10 | loss: 0.00035 | train_rmspe: 0.25766 | valid_rmspe: 0.25914 |  0:04:49s\n",
      "epoch 11 | loss: 0.00035 | train_rmspe: 0.28852 | valid_rmspe: 0.27229 |  0:05:15s\n",
      "epoch 12 | loss: 0.00034 | train_rmspe: 0.25604 | valid_rmspe: 0.26196 |  0:05:41s\n",
      "epoch 13 | loss: 0.00034 | train_rmspe: 0.25263 | valid_rmspe: 0.25632 |  0:06:08s\n",
      "epoch 14 | loss: 0.00034 | train_rmspe: 0.25101 | valid_rmspe: 0.24928 |  0:06:33s\n",
      "epoch 15 | loss: 0.00033 | train_rmspe: 0.25911 | valid_rmspe: 0.25939 |  0:07:00s\n",
      "epoch 16 | loss: 0.00033 | train_rmspe: 0.26478 | valid_rmspe: 0.25885 |  0:07:26s\n",
      "epoch 17 | loss: 0.00033 | train_rmspe: 0.23885 | valid_rmspe: 0.23723 |  0:07:52s\n",
      "epoch 18 | loss: 0.00033 | train_rmspe: 0.23445 | valid_rmspe: 0.23547 |  0:08:18s\n",
      "epoch 19 | loss: 0.00033 | train_rmspe: 0.28329 | valid_rmspe: 0.28138 |  0:08:44s\n",
      "epoch 20 | loss: 0.00033 | train_rmspe: 0.22479 | valid_rmspe: 0.22499 |  0:09:10s\n",
      "epoch 21 | loss: 0.00032 | train_rmspe: 0.22885 | valid_rmspe: 0.23032 |  0:09:37s\n",
      "epoch 22 | loss: 0.00032 | train_rmspe: 0.22883 | valid_rmspe: 0.23182 |  0:10:03s\n",
      "epoch 23 | loss: 0.00032 | train_rmspe: 0.25312 | valid_rmspe: 0.2514  |  0:10:29s\n",
      "epoch 24 | loss: 0.00032 | train_rmspe: 0.24228 | valid_rmspe: 0.24408 |  0:10:55s\n",
      "epoch 25 | loss: 0.00032 | train_rmspe: 0.24669 | valid_rmspe: 0.24578 |  0:11:21s\n",
      "epoch 26 | loss: 0.00032 | train_rmspe: 0.37938 | valid_rmspe: 0.37716 |  0:11:47s\n",
      "epoch 27 | loss: 0.00032 | train_rmspe: 0.24402 | valid_rmspe: 0.24138 |  0:12:14s\n",
      "epoch 28 | loss: 0.00032 | train_rmspe: 0.23246 | valid_rmspe: 0.2305  |  0:12:39s\n",
      "epoch 29 | loss: 0.00032 | train_rmspe: 0.23175 | valid_rmspe: 0.23573 |  0:13:06s\n",
      "epoch 30 | loss: 0.00031 | train_rmspe: 0.22817 | valid_rmspe: 0.22627 |  0:13:32s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_valid_rmspe = 0.22499\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_model_test0.zip\n",
      "----Fold:1--------start----\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.08819 | train_rmspe: 3.4517  | valid_rmspe: 3.0971  |  0:00:25s\n",
      "epoch 1  | loss: 0.00188 | train_rmspe: 1.67306 | valid_rmspe: 1.85881 |  0:00:52s\n",
      "epoch 2  | loss: 0.00093 | train_rmspe: 0.97308 | valid_rmspe: 0.83118 |  0:01:18s\n",
      "epoch 3  | loss: 0.00075 | train_rmspe: 1.19421 | valid_rmspe: 1.37421 |  0:01:45s\n",
      "epoch 4  | loss: 0.00059 | train_rmspe: 0.96481 | valid_rmspe: 1.38233 |  0:02:10s\n",
      "epoch 5  | loss: 0.00055 | train_rmspe: 1.7193  | valid_rmspe: 2.3051  |  0:02:37s\n",
      "epoch 6  | loss: 0.00046 | train_rmspe: 1.32513 | valid_rmspe: 1.53979 |  0:03:03s\n",
      "epoch 7  | loss: 0.00044 | train_rmspe: 0.52911 | valid_rmspe: 0.62361 |  0:03:29s\n",
      "epoch 8  | loss: 0.00043 | train_rmspe: 0.39377 | valid_rmspe: 0.44452 |  0:03:55s\n",
      "epoch 9  | loss: 0.00043 | train_rmspe: 1.02111 | valid_rmspe: 0.93828 |  0:04:21s\n",
      "epoch 10 | loss: 0.00039 | train_rmspe: 0.37057 | valid_rmspe: 0.46746 |  0:04:47s\n",
      "epoch 11 | loss: 0.00038 | train_rmspe: 0.4959  | valid_rmspe: 0.39061 |  0:05:14s\n",
      "epoch 12 | loss: 0.00036 | train_rmspe: 0.34218 | valid_rmspe: 0.31718 |  0:05:40s\n",
      "epoch 13 | loss: 0.00034 | train_rmspe: 0.26091 | valid_rmspe: 0.26281 |  0:06:06s\n",
      "epoch 14 | loss: 0.00034 | train_rmspe: 0.25766 | valid_rmspe: 0.25859 |  0:06:32s\n",
      "epoch 15 | loss: 0.00034 | train_rmspe: 0.30364 | valid_rmspe: 0.30823 |  0:06:58s\n",
      "epoch 16 | loss: 0.00034 | train_rmspe: 0.25797 | valid_rmspe: 0.26013 |  0:07:24s\n",
      "epoch 17 | loss: 0.00033 | train_rmspe: 0.26829 | valid_rmspe: 0.27066 |  0:07:51s\n",
      "epoch 18 | loss: 0.00032 | train_rmspe: 0.23347 | valid_rmspe: 0.23488 |  0:08:17s\n",
      "epoch 19 | loss: 0.00032 | train_rmspe: 0.22734 | valid_rmspe: 0.228   |  0:08:43s\n",
      "epoch 20 | loss: 0.00032 | train_rmspe: 0.2278  | valid_rmspe: 0.23128 |  0:09:09s\n",
      "epoch 21 | loss: 0.00032 | train_rmspe: 0.22559 | valid_rmspe: 0.22963 |  0:09:36s\n",
      "epoch 22 | loss: 0.00032 | train_rmspe: 0.22869 | valid_rmspe: 0.23278 |  0:10:02s\n",
      "epoch 23 | loss: 0.00032 | train_rmspe: 0.22448 | valid_rmspe: 0.22716 |  0:10:28s\n",
      "epoch 24 | loss: 0.00033 | train_rmspe: 0.23047 | valid_rmspe: 0.23129 |  0:10:54s\n",
      "epoch 25 | loss: 0.00032 | train_rmspe: 0.25469 | valid_rmspe: 0.25474 |  0:11:20s\n",
      "epoch 26 | loss: 0.00032 | train_rmspe: 0.22862 | valid_rmspe: 0.22666 |  0:11:46s\n",
      "epoch 27 | loss: 0.00032 | train_rmspe: 0.22565 | valid_rmspe: 0.22635 |  0:12:13s\n",
      "epoch 28 | loss: 0.00032 | train_rmspe: 0.22215 | valid_rmspe: 0.22303 |  0:12:38s\n",
      "epoch 29 | loss: 0.00032 | train_rmspe: 0.21865 | valid_rmspe: 0.22073 |  0:13:05s\n",
      "epoch 30 | loss: 0.00032 | train_rmspe: 0.22734 | valid_rmspe: 0.23046 |  0:13:31s\n",
      "epoch 31 | loss: 0.00032 | train_rmspe: 0.23178 | valid_rmspe: 0.23299 |  0:13:57s\n",
      "epoch 32 | loss: 0.00032 | train_rmspe: 0.21857 | valid_rmspe: 0.22116 |  0:14:23s\n",
      "epoch 33 | loss: 0.00031 | train_rmspe: 0.28388 | valid_rmspe: 0.2854  |  0:14:50s\n",
      "epoch 34 | loss: 0.00032 | train_rmspe: 0.23964 | valid_rmspe: 0.24677 |  0:15:15s\n",
      "epoch 35 | loss: 0.00032 | train_rmspe: 0.22594 | valid_rmspe: 0.22713 |  0:15:42s\n",
      "epoch 36 | loss: 0.00031 | train_rmspe: 0.24115 | valid_rmspe: 0.24385 |  0:16:08s\n",
      "epoch 37 | loss: 0.00032 | train_rmspe: 0.23899 | valid_rmspe: 0.24103 |  0:16:35s\n",
      "epoch 38 | loss: 0.00032 | train_rmspe: 0.2291  | valid_rmspe: 0.23075 |  0:17:00s\n",
      "epoch 39 | loss: 0.00036 | train_rmspe: 0.23416 | valid_rmspe: 0.23599 |  0:17:27s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_valid_rmspe = 0.22073\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_model_test1.zip\n",
      "----Fold:2--------start----\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.0903  | train_rmspe: 1.79143 | valid_rmspe: 2.0572  |  0:00:26s\n",
      "epoch 1  | loss: 0.00106 | train_rmspe: 0.52927 | valid_rmspe: 0.5636  |  0:00:52s\n",
      "epoch 2  | loss: 0.00061 | train_rmspe: 0.41581 | valid_rmspe: 0.43199 |  0:01:19s\n",
      "epoch 3  | loss: 0.0005  | train_rmspe: 0.31199 | valid_rmspe: 0.32585 |  0:01:44s\n",
      "epoch 4  | loss: 0.00045 | train_rmspe: 0.30753 | valid_rmspe: 0.33679 |  0:02:10s\n",
      "epoch 5  | loss: 0.00043 | train_rmspe: 0.36985 | valid_rmspe: 0.39902 |  0:02:36s\n",
      "epoch 6  | loss: 0.00041 | train_rmspe: 0.30717 | valid_rmspe: 0.31836 |  0:03:03s\n",
      "epoch 7  | loss: 0.0004  | train_rmspe: 0.29297 | valid_rmspe: 0.30531 |  0:03:28s\n",
      "epoch 8  | loss: 0.00039 | train_rmspe: 0.26906 | valid_rmspe: 0.27833 |  0:03:55s\n",
      "epoch 9  | loss: 0.00038 | train_rmspe: 0.303   | valid_rmspe: 0.31036 |  0:04:20s\n",
      "epoch 10 | loss: 0.00036 | train_rmspe: 0.25933 | valid_rmspe: 0.2616  |  0:04:47s\n",
      "epoch 11 | loss: 0.00036 | train_rmspe: 0.26058 | valid_rmspe: 0.26426 |  0:05:13s\n",
      "epoch 12 | loss: 0.00035 | train_rmspe: 0.26821 | valid_rmspe: 0.26804 |  0:05:40s\n",
      "epoch 13 | loss: 0.00035 | train_rmspe: 0.26675 | valid_rmspe: 0.2714  |  0:06:06s\n",
      "epoch 14 | loss: 0.00034 | train_rmspe: 0.24258 | valid_rmspe: 0.24985 |  0:06:32s\n",
      "epoch 15 | loss: 0.00034 | train_rmspe: 0.28176 | valid_rmspe: 0.28613 |  0:06:58s\n",
      "epoch 16 | loss: 0.00034 | train_rmspe: 0.23856 | valid_rmspe: 0.24759 |  0:07:25s\n",
      "epoch 17 | loss: 0.00033 | train_rmspe: 0.23141 | valid_rmspe: 0.23816 |  0:07:51s\n",
      "epoch 18 | loss: 0.00033 | train_rmspe: 0.24094 | valid_rmspe: 0.24832 |  0:08:18s\n",
      "epoch 19 | loss: 0.00033 | train_rmspe: 0.2424  | valid_rmspe: 0.24798 |  0:08:43s\n",
      "epoch 20 | loss: 0.00033 | train_rmspe: 0.23758 | valid_rmspe: 0.24399 |  0:09:09s\n",
      "epoch 21 | loss: 0.00033 | train_rmspe: 0.23458 | valid_rmspe: 0.24212 |  0:09:35s\n",
      "epoch 22 | loss: 0.00032 | train_rmspe: 0.23357 | valid_rmspe: 0.24058 |  0:10:02s\n",
      "epoch 23 | loss: 0.00032 | train_rmspe: 0.33524 | valid_rmspe: 0.34026 |  0:10:27s\n",
      "epoch 24 | loss: 0.00032 | train_rmspe: 0.2442  | valid_rmspe: 0.24805 |  0:10:54s\n",
      "epoch 25 | loss: 0.00032 | train_rmspe: 0.28484 | valid_rmspe: 0.28904 |  0:11:19s\n",
      "epoch 26 | loss: 0.00032 | train_rmspe: 0.24088 | valid_rmspe: 0.24933 |  0:11:46s\n",
      "epoch 27 | loss: 0.00032 | train_rmspe: 0.27605 | valid_rmspe: 0.27915 |  0:12:12s\n",
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_valid_rmspe = 0.23816\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_model_test2.zip\n",
      "----Fold:3--------start----\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.09333 | train_rmspe: 1.33721 | valid_rmspe: 1.35833 |  0:00:25s\n",
      "epoch 1  | loss: 0.00118 | train_rmspe: 1.61659 | valid_rmspe: 1.5761  |  0:00:52s\n",
      "epoch 2  | loss: 0.00062 | train_rmspe: 0.4698  | valid_rmspe: 0.486   |  0:01:17s\n",
      "epoch 3  | loss: 0.00052 | train_rmspe: 0.32305 | valid_rmspe: 0.33281 |  0:01:44s\n",
      "epoch 4  | loss: 0.00045 | train_rmspe: 0.33894 | valid_rmspe: 0.34076 |  0:02:09s\n",
      "epoch 5  | loss: 0.00043 | train_rmspe: 0.36639 | valid_rmspe: 0.37813 |  0:02:36s\n",
      "epoch 6  | loss: 0.00041 | train_rmspe: 0.34923 | valid_rmspe: 0.36015 |  0:03:01s\n",
      "epoch 7  | loss: 0.00038 | train_rmspe: 0.33713 | valid_rmspe: 0.33418 |  0:03:28s\n",
      "epoch 8  | loss: 0.00039 | train_rmspe: 0.27545 | valid_rmspe: 0.27971 |  0:03:54s\n",
      "epoch 9  | loss: 0.00038 | train_rmspe: 0.25789 | valid_rmspe: 0.2622  |  0:04:21s\n",
      "epoch 10 | loss: 0.00036 | train_rmspe: 0.27363 | valid_rmspe: 0.27567 |  0:04:46s\n",
      "epoch 11 | loss: 0.00036 | train_rmspe: 0.29195 | valid_rmspe: 0.29119 |  0:05:13s\n",
      "epoch 12 | loss: 0.00035 | train_rmspe: 0.24537 | valid_rmspe: 0.24664 |  0:05:38s\n",
      "epoch 13 | loss: 0.00035 | train_rmspe: 0.27711 | valid_rmspe: 0.27893 |  0:06:05s\n",
      "epoch 14 | loss: 0.00034 | train_rmspe: 0.2474  | valid_rmspe: 0.24861 |  0:06:31s\n",
      "epoch 15 | loss: 0.00033 | train_rmspe: 0.26157 | valid_rmspe: 0.26169 |  0:06:57s\n",
      "epoch 16 | loss: 0.00033 | train_rmspe: 0.2456  | valid_rmspe: 0.24716 |  0:07:22s\n",
      "epoch 17 | loss: 0.00033 | train_rmspe: 0.26868 | valid_rmspe: 0.26935 |  0:07:49s\n",
      "epoch 18 | loss: 0.00032 | train_rmspe: 0.24021 | valid_rmspe: 0.24331 |  0:08:15s\n",
      "epoch 19 | loss: 0.00032 | train_rmspe: 0.2363  | valid_rmspe: 0.23766 |  0:08:41s\n",
      "epoch 20 | loss: 0.00032 | train_rmspe: 0.30923 | valid_rmspe: 0.31041 |  0:09:07s\n",
      "epoch 21 | loss: 0.00032 | train_rmspe: 0.29279 | valid_rmspe: 0.26848 |  0:09:33s\n",
      "epoch 22 | loss: 0.00032 | train_rmspe: 0.23375 | valid_rmspe: 0.23535 |  0:09:59s\n",
      "epoch 23 | loss: 0.00032 | train_rmspe: 0.22951 | valid_rmspe: 0.22981 |  0:10:25s\n",
      "epoch 24 | loss: 0.00032 | train_rmspe: 0.2453  | valid_rmspe: 0.24594 |  0:10:51s\n",
      "epoch 25 | loss: 0.00032 | train_rmspe: 0.22635 | valid_rmspe: 0.22614 |  0:11:17s\n",
      "epoch 26 | loss: 0.00032 | train_rmspe: 0.22864 | valid_rmspe: 0.22989 |  0:11:43s\n",
      "epoch 27 | loss: 0.00032 | train_rmspe: 0.22388 | valid_rmspe: 0.22553 |  0:12:10s\n",
      "epoch 28 | loss: 0.00031 | train_rmspe: 0.25754 | valid_rmspe: 0.24012 |  0:12:35s\n",
      "epoch 29 | loss: 0.00031 | train_rmspe: 0.2176  | valid_rmspe: 0.21873 |  0:13:02s\n",
      "epoch 30 | loss: 0.00031 | train_rmspe: 0.22318 | valid_rmspe: 0.22433 |  0:13:27s\n",
      "epoch 31 | loss: 0.00031 | train_rmspe: 0.22915 | valid_rmspe: 0.22988 |  0:13:54s\n",
      "epoch 32 | loss: 0.00031 | train_rmspe: 0.24476 | valid_rmspe: 0.24513 |  0:14:19s\n",
      "epoch 33 | loss: 0.00034 | train_rmspe: 0.24432 | valid_rmspe: 0.24486 |  0:14:46s\n",
      "epoch 34 | loss: 0.00032 | train_rmspe: 0.24795 | valid_rmspe: 0.2485  |  0:15:11s\n",
      "epoch 35 | loss: 0.00033 | train_rmspe: 0.23099 | valid_rmspe: 0.2319  |  0:15:38s\n",
      "epoch 36 | loss: 0.00031 | train_rmspe: 0.22048 | valid_rmspe: 0.22155 |  0:16:03s\n",
      "epoch 37 | loss: 0.00033 | train_rmspe: 0.24566 | valid_rmspe: 0.24331 |  0:16:30s\n",
      "epoch 38 | loss: 0.00032 | train_rmspe: 0.23368 | valid_rmspe: 0.23452 |  0:16:56s\n",
      "epoch 39 | loss: 0.00031 | train_rmspe: 0.23275 | valid_rmspe: 0.23379 |  0:17:22s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_valid_rmspe = 0.21873\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_model_test3.zip\n",
      "----Fold:4--------start----\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.08507 | train_rmspe: 1.31455 | valid_rmspe: 1.24194 |  0:00:25s\n",
      "epoch 1  | loss: 0.00102 | train_rmspe: 0.60381 | valid_rmspe: 0.56888 |  0:00:51s\n",
      "epoch 2  | loss: 0.00059 | train_rmspe: 0.80584 | valid_rmspe: 0.84222 |  0:01:17s\n",
      "epoch 3  | loss: 0.0005  | train_rmspe: 0.35382 | valid_rmspe: 0.36919 |  0:01:43s\n",
      "epoch 4  | loss: 0.00045 | train_rmspe: 0.2926  | valid_rmspe: 0.29817 |  0:02:10s\n",
      "epoch 5  | loss: 0.00043 | train_rmspe: 0.29082 | valid_rmspe: 0.29477 |  0:02:35s\n",
      "epoch 6  | loss: 0.00042 | train_rmspe: 0.29975 | valid_rmspe: 0.30512 |  0:03:01s\n",
      "epoch 7  | loss: 0.00039 | train_rmspe: 0.29735 | valid_rmspe: 0.30358 |  0:03:27s\n",
      "epoch 8  | loss: 0.00038 | train_rmspe: 0.2741  | valid_rmspe: 0.27864 |  0:03:54s\n",
      "epoch 9  | loss: 0.00036 | train_rmspe: 0.28076 | valid_rmspe: 0.28216 |  0:04:19s\n",
      "epoch 10 | loss: 0.00035 | train_rmspe: 0.26903 | valid_rmspe: 0.27526 |  0:04:46s\n",
      "epoch 11 | loss: 0.00035 | train_rmspe: 0.25529 | valid_rmspe: 0.26517 |  0:05:11s\n",
      "epoch 12 | loss: 0.00034 | train_rmspe: 0.27624 | valid_rmspe: 0.28674 |  0:05:37s\n",
      "epoch 13 | loss: 0.00034 | train_rmspe: 0.26311 | valid_rmspe: 0.27021 |  0:06:03s\n",
      "epoch 14 | loss: 0.00034 | train_rmspe: 0.25708 | valid_rmspe: 0.26234 |  0:06:30s\n",
      "epoch 15 | loss: 0.00033 | train_rmspe: 0.24646 | valid_rmspe: 0.25096 |  0:06:55s\n",
      "epoch 16 | loss: 0.00033 | train_rmspe: 0.27523 | valid_rmspe: 0.27806 |  0:07:21s\n",
      "epoch 17 | loss: 0.00033 | train_rmspe: 0.25646 | valid_rmspe: 0.26451 |  0:07:47s\n",
      "epoch 18 | loss: 0.00033 | train_rmspe: 0.26582 | valid_rmspe: 0.26988 |  0:08:14s\n",
      "epoch 19 | loss: 0.00033 | train_rmspe: 0.25273 | valid_rmspe: 0.25017 |  0:08:39s\n",
      "epoch 20 | loss: 0.00032 | train_rmspe: 0.25254 | valid_rmspe: 0.24592 |  0:09:06s\n",
      "epoch 21 | loss: 0.00032 | train_rmspe: 0.23109 | valid_rmspe: 0.23219 |  0:09:31s\n",
      "epoch 22 | loss: 0.00032 | train_rmspe: 0.24091 | valid_rmspe: 0.24044 |  0:09:57s\n",
      "epoch 23 | loss: 0.00032 | train_rmspe: 0.24588 | valid_rmspe: 0.25524 |  0:10:23s\n",
      "epoch 24 | loss: 0.00032 | train_rmspe: 0.23601 | valid_rmspe: 0.23815 |  0:10:50s\n",
      "epoch 25 | loss: 0.00032 | train_rmspe: 0.27891 | valid_rmspe: 0.27963 |  0:11:15s\n",
      "epoch 26 | loss: 0.00032 | train_rmspe: 0.24072 | valid_rmspe: 0.24235 |  0:11:42s\n",
      "epoch 27 | loss: 0.00032 | train_rmspe: 0.22872 | valid_rmspe: 0.23093 |  0:12:08s\n",
      "epoch 28 | loss: 0.00032 | train_rmspe: 0.2448  | valid_rmspe: 0.24459 |  0:12:34s\n",
      "epoch 29 | loss: 0.00032 | train_rmspe: 0.22057 | valid_rmspe: 0.22191 |  0:13:00s\n",
      "epoch 30 | loss: 0.00032 | train_rmspe: 0.25894 | valid_rmspe: 0.25802 |  0:13:27s\n",
      "epoch 31 | loss: 0.00031 | train_rmspe: 0.2425  | valid_rmspe: 0.24212 |  0:13:52s\n",
      "epoch 32 | loss: 0.00031 | train_rmspe: 0.22495 | valid_rmspe: 0.22751 |  0:14:19s\n",
      "epoch 33 | loss: 0.00031 | train_rmspe: 0.2235  | valid_rmspe: 0.22538 |  0:14:44s\n",
      "epoch 34 | loss: 0.00032 | train_rmspe: 0.23961 | valid_rmspe: 0.24418 |  0:15:11s\n",
      "epoch 35 | loss: 0.00032 | train_rmspe: 0.22692 | valid_rmspe: 0.22749 |  0:15:36s\n",
      "epoch 36 | loss: 0.00032 | train_rmspe: 0.2364  | valid_rmspe: 0.23745 |  0:16:03s\n",
      "epoch 37 | loss: 0.00031 | train_rmspe: 0.21932 | valid_rmspe: 0.22073 |  0:16:28s\n",
      "epoch 38 | loss: 0.00033 | train_rmspe: 0.22453 | valid_rmspe: 0.22612 |  0:16:55s\n",
      "epoch 39 | loss: 0.00033 | train_rmspe: 0.23926 | valid_rmspe: 0.24204 |  0:17:20s\n",
      "epoch 40 | loss: 0.00032 | train_rmspe: 0.27942 | valid_rmspe: 0.28054 |  0:17:47s\n",
      "epoch 41 | loss: 0.00031 | train_rmspe: 0.22889 | valid_rmspe: 0.22944 |  0:18:12s\n",
      "epoch 42 | loss: 0.00033 | train_rmspe: 0.22429 | valid_rmspe: 0.22484 |  0:18:39s\n",
      "epoch 43 | loss: 0.00032 | train_rmspe: 0.25332 | valid_rmspe: 0.24927 |  0:19:05s\n",
      "epoch 44 | loss: 0.00036 | train_rmspe: 0.22671 | valid_rmspe: 0.22824 |  0:19:31s\n",
      "epoch 45 | loss: 0.00031 | train_rmspe: 0.21985 | valid_rmspe: 0.22136 |  0:19:57s\n",
      "epoch 46 | loss: 0.00033 | train_rmspe: 0.23066 | valid_rmspe: 0.23138 |  0:20:23s\n",
      "epoch 47 | loss: 0.00034 | train_rmspe: 0.24847 | valid_rmspe: 0.24941 |  0:20:49s\n",
      "\n",
      "Early stopping occurred at epoch 47 with best_epoch = 37 and best_valid_rmspe = 0.22073\n",
      "Best weights from best epoch are automatically used!\n",
      "Successfully saved model at tabnet_model_test4.zip\n"
     ]
    }
   ],
   "source": [
    "tabnet_params = dict(\n",
    "    n_d = 32,\n",
    "    n_a = 32,\n",
    "    n_steps = 3,\n",
    "    gamma = 1.3,\n",
    "    lambda_sparse = 0,\n",
    "    optimizer_fn = optim.Adam,\n",
    "    optimizer_params = dict(lr = 1e-2, weight_decay = 1e-5),\n",
    "    mask_type = \"entmax\",\n",
    "    scheduler_params = dict(\n",
    "        mode = \"min\", patience = 5, min_lr = 1e-5, factor = 0.9),\n",
    "    scheduler_fn = ReduceLROnPlateau,\n",
    "    seed = 42,\n",
    "    #verbose = 5,\n",
    "    cat_dims=[len(le.classes_)], cat_emb_dim=[10], cat_idxs=[-1] # define categorical features\n",
    ")\n",
    "\n",
    "\n",
    "#  Modeling\n",
    "max_epochs = 50\n",
    "bestscores = []\n",
    "\n",
    "for fold in range(5):\n",
    "    traindf = train[train[\"kfold\"]!=fold].reset_index(drop=True)\n",
    "    validdf = train[train[\"kfold\"]==fold].reset_index(drop=True)\n",
    "\n",
    "    X_train = traindf.drop(['time_id','stock_id','target','kfold','row_id'],axis=1).values\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_traindf = pd.DataFrame(X_train)\n",
    "\n",
    "    X_traindf['stock_id'] = traindf['stock_id']\n",
    "\n",
    "    X_train = X_traindf.values\n",
    "    y_train = traindf['target'].values.reshape(-1,1)\n",
    "\n",
    "    #same for validation\n",
    "    X_valid = validdf.drop(['time_id','stock_id','target','kfold','row_id'],axis=1).values\n",
    "    X_valid = scaler.transform(X_valid)\n",
    "\n",
    "    X_validdf = pd.DataFrame(X_valid)\n",
    "\n",
    "    X_validdf['stock_id'] = validdf['stock_id']\n",
    "    X_valid = X_validdf.values\n",
    "    y_valid = validdf['target'].values.reshape(-1,1)\n",
    "\n",
    "    # calculate weight\n",
    "    y_weight = 1 / np.square(traindf[\"target\"])\n",
    "    \n",
    "    print(\"----Fold:{}--------start----\".format(str(fold)))\n",
    "\n",
    "    # Tabnet Model\n",
    "    clf = TabNetRegressor(**tabnet_params)\n",
    "\n",
    "    #TAbnet training\n",
    "    clf.fit(X_train=X_train,y_train=y_train,\n",
    "            eval_set=[(X_train,y_train),(X_valid,y_valid)],\n",
    "            eval_name=['train','valid'],\n",
    "            eval_metric=[RMSPE], \n",
    "            max_epochs=max_epochs,\n",
    "            patience=10,\n",
    "            batch_size=1024*2,virtual_batch_size=128*2,\n",
    "            #num_workers=4,\n",
    "            drop_last=False,\n",
    "            weights=y_weight,\n",
    "            loss_fn=nn.L1Loss()\n",
    "    )\n",
    "\n",
    "    saving_path_name=\"tabnet_model_test\" + str(fold)\n",
    "    saved_filepath = clf.save_model(saving_path_name)\n",
    "\n",
    "    bestscores.append(clf.best_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ac549490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T00:28:51.871916Z",
     "iopub.status.busy": "2022-03-08T00:28:51.871000Z",
     "iopub.status.idle": "2022-03-08T00:28:51.873840Z",
     "shell.execute_reply": "2022-03-08T00:28:51.873391Z"
    },
    "papermill": {
     "duration": 1.655005,
     "end_time": "2022-03-08T00:28:51.873951",
     "exception": false,
     "start_time": "2022-03-08T00:28:50.218946",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fe = pd.DataFrame() \n",
    " \n",
    "# featurecols = traindf.drop(['row_id', 'target', 'time_id',\"kfold\",\"stock_id\"], axis = 1).columns.to_list()\n",
    "# featurecols.append(\"stock_id\")\n",
    "\n",
    "# Fe[\"features\"] = featurecols\n",
    "# Fe[\"Importance\"] = clf.feature_importances_\n",
    "# Fe  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "784b6007",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T00:28:55.061320Z",
     "iopub.status.busy": "2022-03-08T00:28:55.060443Z",
     "iopub.status.idle": "2022-03-08T00:28:55.063224Z",
     "shell.execute_reply": "2022-03-08T00:28:55.062800Z"
    },
    "papermill": {
     "duration": 1.603555,
     "end_time": "2022-03-08T00:28:55.063339",
     "exception": false,
     "start_time": "2022-03-08T00:28:53.459784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fe2 = Fe.sort_values('Importance',ascending=False)\n",
    "# Fe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b72936e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T00:28:58.240698Z",
     "iopub.status.busy": "2022-03-08T00:28:58.239683Z",
     "iopub.status.idle": "2022-03-08T00:28:58.242758Z",
     "shell.execute_reply": "2022-03-08T00:28:58.242316Z"
    },
    "papermill": {
     "duration": 1.602512,
     "end_time": "2022-03-08T00:28:58.242879",
     "exception": false,
     "start_time": "2022-03-08T00:28:56.640367",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pathB=\"./\"\n",
    "modelpath = [os.path.join(pathB,s) for s in os.listdir(pathB) if (\"zip\" in s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d52c8cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T00:29:01.783401Z",
     "iopub.status.busy": "2022-03-08T00:29:01.782619Z",
     "iopub.status.idle": "2022-03-08T00:29:03.325165Z",
     "shell.execute_reply": "2022-03-08T00:29:03.324698Z"
    },
    "papermill": {
     "duration": 3.24016,
     "end_time": "2022-03-08T00:29:03.325291",
     "exception": false,
     "start_time": "2022-03-08T00:29:00.085131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n"
     ]
    }
   ],
   "source": [
    "train=train.drop(\"kfold\",axis=1)\n",
    "\n",
    "import numpy\n",
    "        \n",
    "for col in train.columns.to_list()[4:]:\n",
    "    test[col] = test[col].fillna(train[col].mean())\n",
    "\n",
    "\n",
    "### normalize ###    \n",
    "\n",
    "x_test = test.drop(['row_id', 'time_id',\"stock_id\"], axis = 1).values\n",
    "    # Transform stock id to a numeric value\n",
    "\n",
    "x_test = scaler.transform(x_test)\n",
    "X_testdf = pd.DataFrame(x_test)\n",
    "\n",
    "X_testdf[\"stock_id\"]=test[\"stock_id\"] \n",
    "\n",
    "# Label encoding\n",
    "X_testdf[\"stock_id\"] = le.transform(X_testdf[\"stock_id\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_test = X_testdf.values\n",
    "\n",
    "    \n",
    "preds = []    \n",
    "for path in modelpath:\n",
    "    \n",
    "    clf.load_model(path)\n",
    "    preds.append(clf.predict(x_test).squeeze(-1))\n",
    "    \n",
    "preds = np.mean(preds,axis=0)\n",
    "\n",
    "\n",
    "test['target'] = preds\n",
    "#test[['row_id', 'target']].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08eaf8fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T00:29:07.065903Z",
     "iopub.status.busy": "2022-03-08T00:29:07.065308Z",
     "iopub.status.idle": "2022-03-08T00:29:07.082951Z",
     "shell.execute_reply": "2022-03-08T00:29:07.082393Z"
    },
    "papermill": {
     "duration": 1.623545,
     "end_time": "2022-03-08T00:29:07.083087",
     "exception": false,
     "start_time": "2022-03-08T00:29:05.459542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001917\n",
       "1   0-32  0.002413\n",
       "2   0-34  0.002413"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str)\n",
    "test_nn[target_name] = (test_predictions_nn+predictions_lgb+preds)/3\n",
    "\n",
    "#score = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\n",
    "#print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
    "\n",
    "display(test_nn[['row_id', target_name]].head(3))\n",
    "test_nn[['row_id', target_name]].to_csv('submission.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3303ad35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T00:29:10.556249Z",
     "iopub.status.busy": "2022-03-08T00:29:10.555395Z",
     "iopub.status.idle": "2022-03-08T00:29:10.558849Z",
     "shell.execute_reply": "2022-03-08T00:29:10.559224Z"
    },
    "papermill": {
     "duration": 1.869096,
     "end_time": "2022-03-08T00:29:10.559370",
     "exception": false,
     "start_time": "2022-03-08T00:29:08.690274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0-4</td>\n",
       "      <td>0.001917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0-32</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0-34</td>\n",
       "      <td>0.002413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  row_id    target\n",
       "0    0-4  0.001917\n",
       "1   0-32  0.002413\n",
       "2   0-34  0.002413"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nn[['row_id', target_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d1e3d2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-08T00:29:13.785855Z",
     "iopub.status.busy": "2022-03-08T00:29:13.785120Z",
     "iopub.status.idle": "2022-03-08T00:29:13.788596Z",
     "shell.execute_reply": "2022-03-08T00:29:13.788153Z"
    },
    "papermill": {
     "duration": 1.643125,
     "end_time": "2022-03-08T00:29:13.788759",
     "exception": false,
     "start_time": "2022-03-08T00:29:12.145634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00177128, 0.0032598 , 0.0032598 ], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15353.079629,
   "end_time": "2022-03-08T00:29:18.117938",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-07T20:13:25.038309",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
